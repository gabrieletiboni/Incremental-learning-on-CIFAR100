{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ablation_Study_Classifiers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqAYJa6pOSLz"
      },
      "source": [
        "**Ablation Study**\n",
        "\n",
        "Classifiers:\n",
        "*   K-Nearest Neighbors (KNN)\n",
        "*   Multi-Layer Perceptron (MLP)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia2BEpYhPcO0"
      },
      "source": [
        "**Install** **requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKlUoA9TINuR"
      },
      "source": [
        "#!pip3 install 'torch==1.3.1'\n",
        "#!pip3 install 'torchvision==0.5.0'\n",
        "#!pip3 install 'Pillow-SIMD'\n",
        "#!pip3 install 'tqdm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw0eY0ZqPnEH"
      },
      "source": [
        "**Import models and functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8YR7ZImPswI"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('./models'):\n",
        "  !git clone https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition.git\n",
        "  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/utils\" \"/content/\"\n",
        "  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/models\" \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p--_UuGQQ-Ht"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDxwYMLlRBiN"
      },
      "source": [
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision.utils\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from models.ResNet import resnet32\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "# Below a modified version that best represents the same ResNet32 used by iCaRL\n",
        "# from models.ResNet_iCaRLVersion import resnet32\n",
        "\n",
        "# from models.iCaRL import *\n",
        "from models.iCaRL import *\n",
        "from utils.utils import *\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G2goFCORZ0g"
      },
      "source": [
        "**GLOBAL PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auywpdVoRc5Z"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "DATA_DIR = './CIFAR_100'\n",
        "RUNS_DIR = '/content/Incremental-learning-on-image-recognition/RUNS'\n",
        "\n",
        "# --- CUSTOM PARAMETERS\n",
        "RANDOM_STATE = 2000          # int or None (Tarantino: 'tarantino', iCaRL: 1993, Telegram: 'telegram')\n",
        "\n",
        "N_GROUPS_FOR_TRAINING = 10   # Numero di gruppi di classi da usare in fase di training (1: usa solo il primo gruppo, 10: usa tutti i gruppi di classi)\n",
        "\n",
        "USE_HERDING = False\n",
        "\n",
        "GITHUB_USER = 0             # 0: Roberto, 1: Alessandro, 2: Gabriele\n",
        "\n",
        "CIFAR_NORMALIZE = False     # If True normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\n",
        "NEIGHBORS = 15              # Number of neighbors considered for KNN\n",
        "\n",
        "X1 = 64                     # Neurons at hidden layer 1\n",
        "X2 = 256                    # Neurons at hidden layer 2\n",
        "\n",
        "METHOD = 'iCARL_KNN'        # 'iCARL_KNN', 'iCARL_MLP'\n",
        "\n",
        "# ---- MLP parameters\n",
        "\n",
        "MLP_BATCH_SIZE = 100\n",
        "MLP_LR = 1e-2\n",
        "MLP_MOMENTUM = 0.9\n",
        "MLP_WEIGHT_DECAY = 1e-5\n",
        "MLP_EPOCHS = 50\n",
        "MLP_STEP_SIZE = 20\n",
        "MLP_GAMMA=0.1\n",
        "\n",
        "# ----------------------\n",
        "\n",
        "DATA_AUGMENTATION = True\n",
        "USE_VALIDATION_SET = False\n",
        "SHUFFLE_CLASSES = True\n",
        "DUMP_FINAL_RESULTS_ON_GSPREADSHEET = True\n",
        "COMMIT_ON_GITHUB = True\n",
        "EVAL_AFTER_EACH_EPOCH = False\n",
        "BCE_VAR = 2          # 1: solo le classi attuali per il one-hot (loss divisa per 128x10, poi 128x20, etc.)\n",
        "                     # 2: usa 100 classi fin da subito nel calcolo della loss (loss divisa sempre per 128x100)\n",
        "                     # 3: usa le classi attuali per il one-hot ma dividi per 128x100 la loss\n",
        "# ----------------------------------\n",
        "\n",
        "# --- HYPERPARAMETERS\n",
        "K = 2000\n",
        "BATCH_SIZE = 128\n",
        "LR = 0.01                   # iCaRL uses LR=2 solo perchè usa la BCE, in generale usare 0.2\n",
        "MOMENTUM = 0.9              # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-5         # Regularization\n",
        "\n",
        "NUM_EPOCHS = 70             # Total number of training epochs (iterations over dataset)\n",
        "DO_MULTILR_STEP_DOWN = True # step down at 7/10 and 9/10\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.2                 # Multiplicative factor for learning rate step-down\n",
        "# ---------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca-HCIiPRv3l"
      },
      "source": [
        "**Define Data Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OEfkd2pRzuE"
      },
      "source": [
        "if CIFAR_NORMALIZE: \n",
        "  MEANS, STDS = (0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "else: \n",
        "  MEANS, STDS = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "\n",
        "# Define transforms for training phase\n",
        "if DATA_AUGMENTATION:\n",
        "\ttrain_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.RandomCrop(32, padding=4),\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.RandomHorizontalFlip(p=0.5),\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\t\t\t\t\t\t\t\t\t])\n",
        "else:\n",
        "\ttrain_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\t\t\t\t\t\t\t\t\t])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100                                                                                                \n",
        "\t\t\t\t\t\t\t\t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0ldk3TkR7w5"
      },
      "source": [
        "**Import dataset CIFAR-100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoD3dnH4R-sC"
      },
      "source": [
        "#For any information about CIFAR-100 follow the link below\n",
        "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "train_dataset = CIFAR100(DATA_DIR, train=True, download=True, transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, train=False, download=False, transform=test_transform)\n",
        "\n",
        "if SHUFFLE_CLASSES:\n",
        "  # --- Shuffle class ordering\n",
        "  if RANDOM_STATE == 'telegram':\n",
        "    classes_indexes = np.array([30,  4, 36, 47, 81, 65, 66, 64, 68, 23, 72, 48, 54, 73,  6, 50, 51,\n",
        "                          83, 75, 88, 58, 62, 39, 60, 94, 25, 84, 37, 33, 76, 34, 57, 46,  3,\n",
        "                          24, 67, 17, 79, 40, 77, 26, 27, 41, 90, 89, 59, 20, 11, 61, 13, 44,\n",
        "                          56,  9, 96, 70, 99, 82, 78,  5, 53, 16, 29,  0, 31,  7, 74, 55, 19,\n",
        "                          42,  1, 92, 63, 52, 69, 22, 18, 28, 35,  8, 91, 86, 32, 97, 98, 15,\n",
        "                            2, 45, 49, 95, 71, 14, 87, 80, 21, 38, 93, 43, 10, 12, 85])\n",
        "    \n",
        "  elif RANDOM_STATE == 'tarantino':\n",
        "    random.seed(653)\n",
        "    classes_indexes = [i for i in range(NUM_CLASSES)]\n",
        "\n",
        "    classes_indexes_cum = []\n",
        "    remaining = [i for i in range(NUM_CLASSES)]\n",
        "    for i in range(10):\n",
        "      classes_indexes_cum += random.sample(remaining, 10)\n",
        "      remaining = list(set(classes_indexes)-set(classes_indexes_cum))\n",
        "\n",
        "    classes_indexes = classes_indexes_cum\n",
        "    classes_indexes = np.array(classes_indexes)\n",
        "\n",
        "    print('Tarantino classes order:', classes_indexes)\n",
        "\n",
        "  else:\n",
        "    if RANDOM_STATE is not None:\n",
        "      np.random.seed(RANDOM_STATE)\n",
        "\n",
        "    classes_indexes = np.array([i for i in range(NUM_CLASSES)])\n",
        "    np.random.shuffle(classes_indexes)\n",
        "\n",
        "\n",
        "  classes_shuffle_dict = {ind:i for i, ind in enumerate(classes_indexes)}\n",
        "\n",
        "  train_dataset.targets = [classes_shuffle_dict[tar] for tar in train_dataset.targets]\n",
        "  test_dataset.targets = [classes_shuffle_dict[tar] for tar in test_dataset.targets]\n",
        "\n",
        "  CLASSES = train_dataset.classes\n",
        "  train_dataset.class_to_idx = {CLASSES[i]:ind for i,ind in enumerate(classes_indexes)}\n",
        "  LABEL_INDEX_DICT = train_dataset.class_to_idx\n",
        "else:\n",
        "  CLASSES = train_dataset.classes\n",
        "  LABEL_INDEX_DICT = train_dataset.class_to_idx\n",
        "\n",
        "# show_random_images(train_dataset, 5, mean=MEANS, std=STDS)\n",
        "\n",
        "print('Train Dataset length:', len(train_dataset))\n",
        "print('Test Dataset length:', len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnrDCCw0WjVE"
      },
      "source": [
        "**3-Layers MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJYMWqETWnuQ"
      },
      "source": [
        "### 3-layers MLP\n",
        "\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch\n",
        "\n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            # hidden layer 1\n",
        "            nn.Linear(64, X1),\n",
        "            nn.ReLU(),\n",
        "            # hidden layer 2\n",
        "            nn.Linear(X1, X2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(X2, num_classes)\n",
        "        ) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "def MLP(**kwargs):\n",
        "  model = Perceptron(**kwargs)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0pqx9zpSLW-"
      },
      "source": [
        "**Prepare training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UBoGr_XSNqA"
      },
      "source": [
        "net = resnet32(num_classes=NUM_CLASSES)\n",
        "icarl = iCaRL(device=DEVICE, batch_size=BATCH_SIZE, K=K, dataset=train_dataset)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='mean') # reduction='sum' is crucial as BCE is designed for one output neuron only (it averages on batch_size*num_classes instead of on just batch_size) - actually this is why iCaRL keeps a really high learning rate\n",
        "criterion_eval = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd1q3546Saez"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-6Ap1LDShRD"
      },
      "source": [
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "val_indexes_cum = []\n",
        "test_indexes_cum = []\n",
        "current_classes_cum = []\n",
        "\n",
        "group_losses_train = []\n",
        "group_losses_eval = []\n",
        "group_accuracies_train = []\n",
        "group_accuracies_eval = []\n",
        "group_accuracies_eval_curr = []\n",
        "group_accuracies_eval_KNN = []\n",
        "group_accuracies_eval_MLP = []\n",
        "\n",
        "now = datetime.datetime.now(timezone('Europe/Rome'))\n",
        "CURRENT_RUN = 'RUN_' + now.strftime(\"%Y-%m-%d %H %M %S\")\n",
        "try:\n",
        "  os.makedirs(RUNS_DIR+'/'+CURRENT_RUN)\n",
        "except OSError:\n",
        "  print (\"FATAL ERROR - Creation of the directory of the current run failed\")\n",
        "  sys.exit()\n",
        "\n",
        "dump_hyperparameters(path=RUNS_DIR+'/'+CURRENT_RUN, lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, method=METHOD, batch_size=BATCH_SIZE)\n",
        "\n",
        "START_TIME = time.time()\n",
        "\n",
        "for group_number in range(N_GROUPS_FOR_TRAINING):\n",
        "\n",
        "  starting_label = (group_number*10)\n",
        "  ending_label = (group_number+1)*10\n",
        "  current_classes = list(range(starting_label, ending_label))\n",
        "\n",
        "  new_indexes = get_indexes_from_labels(train_dataset, current_classes)\n",
        "\n",
        "  # np.random.shuffle(new_indexes)\n",
        "\n",
        "  train_dataset_curr = Subset(train_dataset, new_indexes)\n",
        "  exemplars = icarl.flattened_exemplars()\n",
        "  train_dataset_cum_exemplars = Subset(train_dataset, exemplars+new_indexes)\n",
        "\n",
        "  # Update training set\n",
        "  train_dataloader = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "  train_dataloader_cum_exemplars = DataLoader(train_dataset_cum_exemplars, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "  train_dataloader_for_evaluation = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "  train_dataloader_cum_exemplars_for_evaluation = DataLoader(train_dataset_cum_exemplars, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "  # Update test set\n",
        "  new_test_indexes = get_indexes_from_labels(test_dataset, current_classes)\n",
        "  test_dataset_cum = Subset(test_dataset, test_indexes_cum+new_test_indexes)\n",
        "  test_dataset_curr = Subset(test_dataset, new_test_indexes)\n",
        "\n",
        "  test_indexes_cum += new_test_indexes\n",
        "\n",
        "  test_dataloader = DataLoader(test_dataset_cum, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "  test_dataloader_curr = DataLoader(test_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "  print('******************************')\n",
        "  print(f'NEW GROUP OF CLASSES {(group_number+1)}°/{N_GROUPS_FOR_TRAINING}')\n",
        "  print('Training set length:', len(train_dataset_curr))\n",
        "  if USE_VALIDATION_SET:\n",
        "    print('Validation set length:', len(val_dataset_cum))\n",
        "  print('Test set length:', len(test_dataset_cum))\n",
        "  \n",
        "  net = net.to(DEVICE)\n",
        "\n",
        "  parameters_to_optimize = net.parameters()\n",
        "\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  milestone_1 = math.floor(NUM_EPOCHS/10*7)\n",
        "  milestone_2 = math.floor(NUM_EPOCHS/10*9)\n",
        "\n",
        "  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[milestone_1, milestone_2], gamma=GAMMA)\n",
        "\n",
        "  current_step = 0\n",
        "  losses_train = []\n",
        "  losses_eval = []\n",
        "  accuracies_train = []\n",
        "  accuracies_eval = []\n",
        "  accuracies_eval_curr = []\n",
        "  accuracies_eval_KNN = []\n",
        "  accuracies_eval_MLP = []\n",
        "\n",
        "  net_old = None\n",
        "  if starting_label > 0:\n",
        "    # Salva la rete attuale per calcolare i vecchi outputs\n",
        "    net_old = deepcopy(net)\n",
        "\n",
        "  net.train()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    #\n",
        "    # Update weights using iCaRL BCE and distillation loss on Dataset\n",
        "    #\n",
        "    loss = icarl.update_representation(net, net_old, train_dataloader_cum_exemplars, criterion, optimizer, current_classes, starting_label, ending_label, current_step, bce_var=BCE_VAR)\n",
        "\n",
        "    current_step += 1\n",
        "    scheduler.step()\n",
        "\n",
        "    print('--- Epoch {}, Loss on train: {}'.format(epoch+1, loss.item()))\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "\n",
        "  # --- END OF TRAINING FOR THIS GROUP OF CLASSES\n",
        "  print('Length on train dataset (exemplars included):', len(train_dataset_cum_exemplars))\n",
        "\n",
        "  #\n",
        "  # Compute means of each class using the entire current training set and the exemplars\n",
        "  #\n",
        "  icarl.compute_means(net, train_dataloader_cum_exemplars, ending_label)\n",
        "\n",
        "  if starting_label > 0:\n",
        "    #\n",
        "    # Reduce number of exemplars for each class to 2000/ending_label\n",
        "    #\n",
        "    icarl.reduce_exemplars(starting_label, ending_label)\n",
        " \n",
        "  #\n",
        "  # Construct exemplars for future evaluation\n",
        "  #\n",
        "  icarl.construct_exemplars(net, starting_label, ending_label, herding=USE_HERDING)\n",
        "  exemplars = icarl.flattened_exemplars()\n",
        "  if METHOD == 'iCARL_KNN':\n",
        "    exemplars_KNN = Subset(train_dataset, exemplars)\n",
        "    KNN_dataloader = DataLoader(exemplars_KNN, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "  if METHOD == 'iCARL_MLP':\n",
        "    exemplars_MLP = Subset(train_dataset, exemplars)\n",
        "    MLP_dataloader = DataLoader(exemplars_MLP, batch_size=MLP_BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, accuracy_test = eval_model(net, test_dataloader, criterion=criterion_eval,\n",
        "                                          dataset_length=len(test_dataset_cum), use_bce_loss=None,\n",
        "                                          ending_label=ending_label, loss=False, device=DEVICE, display=True, suffix=' (group)')\n",
        "  losses_eval.append(-1)\n",
        "  accuracies_eval.append(accuracy_test)\n",
        "\n",
        "  if METHOD == 'iCARL_KNN':\n",
        "    #\n",
        "    # Eval model using KNN on test set\n",
        "    #\n",
        "    with torch.no_grad():\n",
        "      accuracy_eval_KNN = icarl.eval_model_KNN(net, KNN_dataloader, test_dataloader, dataset_length=len(test_dataset_cum), display=True, suffix=' (group)', k=NEIGHBORS)\n",
        "    accuracies_eval_KNN.append(accuracy_eval_KNN)\n",
        "  if METHOD == 'iCARL_MLP':\n",
        "    #\n",
        "    # Eval model using MLP on test set\n",
        "    #\n",
        "    with torch.no_grad():\n",
        "      accuracy_eval_MLP = icarl.eval_model_MLP(net, MLP_dataloader, test_dataloader, dataset_length=len(test_dataset_cum), display=True, suffix=' (group)', lr=MLP_LR, bs=MLP_BATCH_SIZE, momentum=MLP_MOMENTUM, wd=MLP_WEIGHT_DECAY, num_epochs=MLP_EPOCHS,step_size=MLP_STEP_SIZE, gamma=MLP_GAMMA, nc=ending_label)\n",
        "    accuracies_eval_MLP.append(accuracy_eval_MLP)\n",
        "\n",
        "  #\n",
        "  # Accuracy on training\n",
        "  #\n",
        "  with torch.no_grad():\n",
        "    accuracy_train = eval_model_accuracy(net, train_dataloader_for_evaluation, dataset_length=len(train_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='train (group)')\n",
        "  accuracies_train.append(accuracy_train)\n",
        "\n",
        "  #\n",
        "  # Compute accuracy on test for novel classes only\n",
        "  #\n",
        "  with torch.no_grad():\n",
        "    accuracy_eval_curr_classes = eval_model_accuracy(net, test_dataloader_curr, dataset_length=len(test_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='test novel classes (group)')\n",
        "  accuracies_eval_curr.append(accuracy_eval_curr_classes)\n",
        "\n",
        "  path = RUNS_DIR+'/'+CURRENT_RUN    \n",
        "  create_dir_for_current_group(group_number, path=path)\n",
        "  \n",
        "  draw_graphs(losses_train,\n",
        "        losses_eval,\n",
        "        accuracies_train,\n",
        "        accuracies_eval,\n",
        "        num_epochs=NUM_EPOCHS, use_validation=USE_VALIDATION_SET, print_img=False, save=True, path=path, group_number=group_number)\n",
        "  \n",
        "  dump_to_csv(losses_train,\n",
        "        losses_eval,\n",
        "        accuracies_train,\n",
        "        accuracies_eval,\n",
        "        group_number=group_number, path=path)\n",
        "\n",
        "  group_losses_train.append(losses_train[-1])\n",
        "  group_losses_eval.append(losses_eval[-1])\n",
        "  group_accuracies_train.append(accuracies_train[-1])\n",
        "  group_accuracies_eval.append(accuracies_eval[-1])\n",
        "  group_accuracies_eval_curr.append(accuracies_eval_curr[-1])\n",
        "  if METHOD == 'iCARL_KNN':\n",
        "    group_accuracies_eval_KNN.append(accuracies_eval_KNN[-1])\n",
        "  if METHOD == 'iCARL_MLP':\n",
        "    group_accuracies_eval_MLP.append(accuracies_eval_MLP[-1])\n",
        "\n",
        "# END OF OVERALL TRAINING\n",
        "if METHOD == 'iCARL_KNN':\n",
        "  dump_final_values_nme(group_losses_train, group_accuracies_train, group_accuracies_eval_KNN, group_accuracies_eval, group_accuracies_eval_curr, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "  draw_final_graphs_nme(group_losses_train, group_accuracies_eval_KNN, group_accuracies_eval, use_validation=USE_VALIDATION_SET, print_img=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "\n",
        "  print('Average incremental accuracy (KNN)', np.mean(group_accuracies_eval_KNN))\n",
        "\n",
        "if METHOD == 'iCARL_MLP':\n",
        "  dump_final_values_nme(group_losses_train, group_accuracies_train, group_accuracies_eval_MLP, group_accuracies_eval, group_accuracies_eval_curr, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "  draw_final_graphs_nme(group_losses_train, group_accuracies_eval_MLP, group_accuracies_eval, use_validation=USE_VALIDATION_SET, print_img=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "\n",
        "  print('Average incremental accuracy (MLP)', np.mean(group_accuracies_eval_MLP))\n",
        "\n",
        "print('Average incremental accuracy (hybrid 1)', np.mean(group_accuracies_eval))\n",
        "\n",
        "#\n",
        "# Compute and display confusion matrix\n",
        "#\n",
        "#conf_mat = get_conf_matrix_nme(net, test_dataloader, icarl=icarl, ending_label=ending_label, device=DEVICE)\n",
        "#display_conf_matrix(conf_mat, display=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "\n",
        "DURATION = round((time.time()-START_TIME)/60, 1)\n",
        "print(f\"> In {(DURATION)} minutes\")\n",
        "\n",
        "github_link = 'https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition/tree/master/RUNS/'+str(CURRENT_RUN)\n",
        "github_link = github_link.replace(\" \", \"%20\")\n",
        "hyperparameters_string = get_hyperparameter_string(lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, multilrstep=DO_MULTILR_STEP_DOWN, gamma=GAMMA)\n",
        "if DUMP_FINAL_RESULTS_ON_GSPREADSHEET:\n",
        "  if METHOD == 'iCARL_KNN':\n",
        "    dump_on_gspreadsheet_nme(CURRENT_RUN, GITHUB_USER, github_link, METHOD, RANDOM_STATE, USE_HERDING,\n",
        "                            CIFAR_NORMALIZE, BCE_VAR, group_losses_train, group_accuracies_train, group_accuracies_eval_KNN,\n",
        "                            group_accuracies_eval, group_accuracies_eval_curr, DURATION,\n",
        "                            hyperparameters=hyperparameters_string, ablation='clf', params='K = {}'.format(NEIGHBORS))\n",
        "  if METHOD == 'iCARL_MLP':\n",
        "    dump_on_gspreadsheet_nme(CURRENT_RUN, GITHUB_USER, github_link, METHOD, RANDOM_STATE, USE_HERDING,\n",
        "                            CIFAR_NORMALIZE, BCE_VAR, group_losses_train, group_accuracies_train, group_accuracies_eval_MLP,\n",
        "                            group_accuracies_eval, group_accuracies_eval_curr, DURATION,\n",
        "                            hyperparameters=hyperparameters_string, ablation='clf', params='MLP')\n",
        "\n",
        "beep()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}