{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LwF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fab5dd4609674628bd70e309c107bac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9499b0b4109a4677999f8488bd403dd0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5bb7bf8c9b704876bfdce78633837ae4",
              "IPY_MODEL_7da92e17575d4eda8719fdaf7d52adf8"
            ]
          }
        },
        "9499b0b4109a4677999f8488bd403dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bb7bf8c9b704876bfdce78633837ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_94aba0b9f21e44cfa182894e17935d49",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24b8f3a2c1c44829bba25c6b5120a828"
          }
        },
        "7da92e17575d4eda8719fdaf7d52adf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_894b968ab08f42849488ecad89391c9e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:30&lt;00:00, 14152631.89it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bfd422b31ff4414aa99c9e51a0208aaa"
          }
        },
        "94aba0b9f21e44cfa182894e17935d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24b8f3a2c1c44829bba25c6b5120a828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "894b968ab08f42849488ecad89391c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bfd422b31ff4414aa99c9e51a0208aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGeu47FfPs5I",
        "colab_type": "text"
      },
      "source": [
        "### **Learning without Forgetting (LwF)**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CwlqKl4RzLi",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip3 install 'torch==1.3.1'\n",
        "#!pip3 install 'torchvision==0.5.0'\n",
        "#!pip3 install 'Pillow-SIMD'\n",
        "#!pip3 install 'tqdm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrkE-2MVbZjU",
        "colab_type": "text"
      },
      "source": [
        "**Import models and functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOGd-aASa40O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "02b8753c-78f2-4237-edb3-d0646f692971"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('./models'):\n",
        "  !git clone https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition.git\n",
        "  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/utils\" \"/content/\"\n",
        "  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/models\" \"/content/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Incremental-learning-on-image-recognition'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 1844 (delta 11), reused 31 (delta 8), pack-reused 1805\u001b[K\n",
            "Receiving objects: 100% (1844/1844), 8.09 MiB | 4.23 MiB/s, done.\n",
            "Resolving deltas: 100% (511/511), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZgBn2rBV9L",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDs3TD9Bda1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision.utils\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from models.ResNet import resnet32\n",
        "from copy import deepcopy\n",
        "\n",
        "# import custom Learning without Forgetting (LwF) implementation\n",
        "from models.LwF import *\n",
        "from utils.utils import *\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT0Un92iJHFo",
        "colab_type": "text"
      },
      "source": [
        "**GLOBAL PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSHzmwaxJGEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "DATA_DIR = './CIFAR_100'\n",
        "RUNS_DIR = '/content/Incremental-learning-on-image-recognition/RUNS'\n",
        "\n",
        "# --- CUSTOM PARAMETERS\n",
        "RANDOM_STATE = 90           # int or None (Tarantino: 'tarantino', iCaRL: 1993, Telegram: 'telegram')\n",
        "\n",
        "N_GROUPS_FOR_TRAINING = 10  # Numero di gruppi di classi da usare in fase di training (1: usa solo il primo gruppo, 10: usa tutti i gruppi di classi)\n",
        "\n",
        "GITHUB_USER = 0             # 0: Roberto, 1: Alessandro, 2: Gabriele\n",
        "\n",
        "CIFAR_NORMALIZE = False     # If True normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\n",
        "METHOD = 'LwF'\n",
        "# ---------------------\n",
        "\n",
        "DATA_AUGMENTATION = True\n",
        "USE_VALIDATION_SET = False\n",
        "SHUFFLE_CLASSES = True\n",
        "DUMP_FINAL_RESULTS_ON_GSPREADSHEET = True\n",
        "COMMIT_ON_GITHUB = True\n",
        "EVAL_AFTER_EACH_EPOCH = False\n",
        "BCE_VAR = 2          # 1: solo le classi attuali per il one-hot (loss divisa per 128x10, poi 128x20, etc.)\n",
        "                     # 2: usa 100 classi fin da subito nel calcolo della loss (loss divisa sempre per 128x100)\n",
        "                     # 3: usa le classi attuali per il one-hot ma dividi per 128x100 la loss\n",
        "\n",
        "# --- HYPERPARAMETERS\n",
        "K = 2000\n",
        "BATCH_SIZE = 128\n",
        "LR = 2.                     # iCaRL uses LR=2 solo perchè usa la BCE, in generale usare 0.2\n",
        "MOMENTUM = 0.9              # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-5         # Regularization\n",
        "\n",
        "NUM_EPOCHS = 70             # Total number of training epochs (iterations over dataset)\n",
        "DO_MULTILR_STEP_DOWN = True # step down at 7/10 and 9/10\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.2                 # Multiplicative factor for learning rate step-down\n",
        "# ---------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As6ukzaJE0kN",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e19l7N4HE6EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if CIFAR_NORMALIZE: \n",
        "  MEANS, STDS = (0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "else: \n",
        "  MEANS, STDS = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "\n",
        "# Define transforms for training phase\n",
        "if DATA_AUGMENTATION:\n",
        "\ttrain_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.RandomCrop(32, padding=4),\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.RandomHorizontalFlip(p=0.5),\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\t\t\t\t\t\t\t\t\t])\n",
        "else:\n",
        "\ttrain_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\t\t\t\t\t\t\t\t\t])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100                                                                                                \n",
        "\t\t\t\t\t\t\t\t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVnFkK3mRhZG",
        "colab_type": "text"
      },
      "source": [
        "**Import dataset CIFAR-100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_acReX5Rhkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "fab5dd4609674628bd70e309c107bac3",
            "9499b0b4109a4677999f8488bd403dd0",
            "5bb7bf8c9b704876bfdce78633837ae4",
            "7da92e17575d4eda8719fdaf7d52adf8",
            "94aba0b9f21e44cfa182894e17935d49",
            "24b8f3a2c1c44829bba25c6b5120a828",
            "894b968ab08f42849488ecad89391c9e",
            "bfd422b31ff4414aa99c9e51a0208aaa"
          ]
        },
        "outputId": "8bfd3c98-84a5-4ca2-a818-aee3bbe2c8f3"
      },
      "source": [
        "#For any information about CIFAR-100 follow the link below\n",
        "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "train_dataset = CIFAR100(DATA_DIR, train=True, download=True, transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, train=False, download=False, transform=test_transform)\n",
        "\n",
        "if SHUFFLE_CLASSES:\n",
        "  # --- Shuffle class ordering\n",
        "  if RANDOM_STATE == 'telegram':\n",
        "    classes_indexes = np.array([30,  4, 36, 47, 81, 65, 66, 64, 68, 23, 72, 48, 54, 73,  6, 50, 51,\n",
        "                          83, 75, 88, 58, 62, 39, 60, 94, 25, 84, 37, 33, 76, 34, 57, 46,  3,\n",
        "                          24, 67, 17, 79, 40, 77, 26, 27, 41, 90, 89, 59, 20, 11, 61, 13, 44,\n",
        "                          56,  9, 96, 70, 99, 82, 78,  5, 53, 16, 29,  0, 31,  7, 74, 55, 19,\n",
        "                          42,  1, 92, 63, 52, 69, 22, 18, 28, 35,  8, 91, 86, 32, 97, 98, 15,\n",
        "                            2, 45, 49, 95, 71, 14, 87, 80, 21, 38, 93, 43, 10, 12, 85])\n",
        "    \n",
        "  elif RANDOM_STATE == 'tarantino':\n",
        "    random.seed(653)\n",
        "    classes_indexes = [i for i in range(NUM_CLASSES)]\n",
        "\n",
        "    classes_indexes_cum = []\n",
        "    remaining = [i for i in range(NUM_CLASSES)]\n",
        "    for i in range(10):\n",
        "      classes_indexes_cum += random.sample(remaining, 10)\n",
        "      remaining = list(set(classes_indexes)-set(classes_indexes_cum))\n",
        "\n",
        "    classes_indexes = classes_indexes_cum\n",
        "    classes_indexes = np.array(classes_indexes)\n",
        "\n",
        "    print('Tarantino classes order:', classes_indexes)\n",
        "\n",
        "  else:\n",
        "    if RANDOM_STATE is not None:\n",
        "      np.random.seed(RANDOM_STATE)\n",
        "\n",
        "    classes_indexes = np.array([i for i in range(NUM_CLASSES)])\n",
        "    np.random.shuffle(classes_indexes)\n",
        "\n",
        "\n",
        "  classes_shuffle_dict = {ind:i for i, ind in enumerate(classes_indexes)}\n",
        "\n",
        "  train_dataset.targets = [classes_shuffle_dict[tar] for tar in train_dataset.targets]\n",
        "  test_dataset.targets = [classes_shuffle_dict[tar] for tar in test_dataset.targets]\n",
        "\n",
        "  CLASSES = train_dataset.classes\n",
        "  train_dataset.class_to_idx = {CLASSES[i]:ind for i,ind in enumerate(classes_indexes)}\n",
        "  LABEL_INDEX_DICT = train_dataset.class_to_idx\n",
        "else:\n",
        "  CLASSES = train_dataset.classes\n",
        "  LABEL_INDEX_DICT = train_dataset.class_to_idx\n",
        "\n",
        "# show_random_images(train_dataset, 5, mean=MEANS, std=STDS)\n",
        "\n",
        "print('Train Dataset length:', len(train_dataset))\n",
        "print('Test Dataset length:', len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./CIFAR_100/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fab5dd4609674628bd70e309c107bac3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./CIFAR_100/cifar-100-python.tar.gz to ./CIFAR_100\n",
            "Train Dataset length: 50000\n",
            "Test Dataset length: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4ZhjI56LOIt",
        "colab_type": "text"
      },
      "source": [
        "**Prepare training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f70xiaS4LNo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = resnet32(num_classes=NUM_CLASSES)\n",
        "lwf = LwF(device=DEVICE, batch_size=BATCH_SIZE, K=K, dataset=train_dataset)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='mean') # reduction='sum' is crucial as BCE is designed for one output neuron only (it averages on batch_size*num_classes instead of on just batch_size) - actually this is why iCaRL keeps a really high learning rate\n",
        "criterion_eval = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFXEmHyHo-Zt",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaHEiGnro_85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "077c3345-3d18-49c8-cf04-11cc84712530"
      },
      "source": [
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "val_indexes_cum = []\n",
        "test_indexes_cum = []\n",
        "current_classes_cum = []\n",
        "\n",
        "group_losses_train = []\n",
        "group_losses_eval = []\n",
        "group_accuracies_train = []\n",
        "group_accuracies_eval = []\n",
        "group_accuracies_eval_curr = []\n",
        "\n",
        "now = datetime.datetime.now(timezone('Europe/Rome'))\n",
        "CURRENT_RUN = 'RUN_' + now.strftime(\"%Y-%m-%d %H %M %S\")\n",
        "try:\n",
        "  os.makedirs(RUNS_DIR+'/'+CURRENT_RUN)\n",
        "except OSError:\n",
        "  raise RuntimeError(\"Creation of the directory of the current run failed\")\n",
        "\n",
        "dump_hyperparameters(path=RUNS_DIR+'/'+CURRENT_RUN, lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, method=METHOD, batch_size=BATCH_SIZE)\n",
        "\n",
        "START_TIME = time.time()\n",
        "\n",
        "for group_number in range(N_GROUPS_FOR_TRAINING):\n",
        "\n",
        "  starting_label = (group_number*10)\n",
        "  ending_label = (group_number+1)*10\n",
        "  current_classes = list(range(starting_label, ending_label))\n",
        "\n",
        "  new_indexes = get_indexes_from_labels(train_dataset, current_classes)\n",
        "\n",
        "  # np.random.shuffle(new_indexes)\n",
        "\n",
        "  train_dataset_curr = Subset(train_dataset, new_indexes)\n",
        "\n",
        "  # Update training set\n",
        "  train_dataloader = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "  train_dataloader_for_evaluation = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4) # the only difference is the shuffle=False because we are evaluating\n",
        "\n",
        "  # Update test set\n",
        "  new_test_indexes = get_indexes_from_labels(test_dataset, current_classes)\n",
        "  test_dataset_cum = Subset(test_dataset, test_indexes_cum+new_test_indexes) # cumulative (contains samples of all classes seen until now)\n",
        "  test_dataset_curr = Subset(test_dataset, new_test_indexes) # current (contains samples of current classes)\n",
        "\n",
        "  test_indexes_cum += new_test_indexes\n",
        "  # test dataloaders\n",
        "  test_dataloader = DataLoader(test_dataset_cum, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "  test_dataloader_curr = DataLoader(test_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "  print('******************************')\n",
        "  print(f'NEW GROUP OF CLASSES {(group_number+1)}°/{N_GROUPS_FOR_TRAINING}')\n",
        "  print('Training set length:', len(train_dataset_curr))\n",
        "  if USE_VALIDATION_SET:\n",
        "    print('Validation set length:', len(val_dataset_cum))\n",
        "  print('Test set length:', len(test_dataset_cum))\n",
        "  \n",
        "  net = net.to(DEVICE)\n",
        "\n",
        "  parameters_to_optimize = net.parameters()\n",
        "\n",
        "  # LR decay policy\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  milestone_1 = math.floor(NUM_EPOCHS/10*7)\n",
        "  milestone_2 = math.floor(NUM_EPOCHS/10*9)\n",
        "\n",
        "  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[milestone_1, milestone_2], gamma=GAMMA)\n",
        "\n",
        "  current_step = 0\n",
        "  losses_train = []\n",
        "  losses_eval = []\n",
        "  accuracies_train = []\n",
        "  accuracies_eval = []\n",
        "  accuracies_eval_curr = []\n",
        "\n",
        "  net_old = None\n",
        "  if starting_label > 0:\n",
        "    # Salva la rete attuale per calcolare i vecchi outputs\n",
        "    net_old = deepcopy(net)\n",
        "\n",
        "  net.train()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    #\n",
        "    # Update weights using lwf BCE and distillation loss on Dataset\n",
        "    #\n",
        "    loss = lwf.update_representation(net, net_old, train_dataloader, criterion, optimizer, current_classes, starting_label, ending_label, current_step, bce_var=BCE_VAR)\n",
        "\n",
        "    current_step += 1\n",
        "    scheduler.step()\n",
        "\n",
        "    print('--- Epoch {}, Loss on train: {}'.format(epoch+1, loss.item()))\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "\n",
        "  # --- END OF TRAINING FOR THIS GROUP OF CLASSES\n",
        "  print('Length on train dataset:', len(train_dataset))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, accuracy_test = eval_model(net, test_dataloader, criterion=criterion_eval,\n",
        "                                          dataset_length=len(test_dataset_cum), use_bce_loss=True,\n",
        "                                          ending_label=ending_label, loss=False, device=DEVICE, display=True, suffix=' (group)')\n",
        "  losses_eval.append(-1)\n",
        "  accuracies_eval.append(accuracy_test)\n",
        "\n",
        "  #\n",
        "  # Accuracy on training\n",
        "  #\n",
        "  with torch.no_grad():\n",
        "    accuracy_train = eval_model_accuracy(net, train_dataloader_for_evaluation, dataset_length=len(train_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='train (group)')\n",
        "  accuracies_train.append(accuracy_train)\n",
        "\n",
        "  #\n",
        "  # Compute accuracy on test for novel classes only\n",
        "  #\n",
        "  with torch.no_grad():\n",
        "    accuracy_eval_curr_classes = eval_model_accuracy(net, test_dataloader_curr, dataset_length=len(test_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='test novel classes (group)')\n",
        "  accuracies_eval_curr.append(accuracy_eval_curr_classes)\n",
        "\n",
        "  path = RUNS_DIR+'/'+CURRENT_RUN    \n",
        "  create_dir_for_current_group(group_number, path=path)\n",
        "\n",
        "  draw_graphs(losses_train,\n",
        "        losses_eval,\n",
        "        accuracies_train,\n",
        "        accuracies_eval,\n",
        "        num_epochs=NUM_EPOCHS, use_validation=USE_VALIDATION_SET, print_img=False, save=True, path=path, group_number=group_number)\n",
        "  \n",
        "  dump_to_csv(losses_train,\n",
        "        losses_eval,\n",
        "        accuracies_train,\n",
        "        accuracies_eval,\n",
        "        group_number=group_number, path=path)\n",
        "\n",
        "  group_losses_train.append(losses_train[-1])\n",
        "  group_losses_eval.append(losses_eval[-1])\n",
        "  group_accuracies_train.append(accuracies_train[-1])\n",
        "  group_accuracies_eval.append(accuracies_eval[-1])\n",
        "  group_accuracies_eval_curr.append(accuracies_eval_curr[-1])\n",
        "\n",
        "# END OF OVERALL TRAINING\n",
        "dump_final_values(group_losses_train, group_losses_eval, group_accuracies_train, group_accuracies_eval, group_accuracies_eval_curr, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "draw_final_graphs(group_losses_train, group_losses_eval, group_accuracies_eval, group_accuracies_eval_curr, is_joint_training=False, use_validation=USE_VALIDATION_SET, print_img=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "\n",
        "print('Average incremental accuracy', np.mean(group_accuracies_eval))\n",
        "\n",
        "#\n",
        "# Compute and display confusion matrix\n",
        "#\n",
        "\n",
        "conf_mat = get_conf_matrix(net, test_dataloader, ending_label=ending_label, device=DEVICE)\n",
        "display_conf_matrix(conf_mat, display=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "\n",
        "DURATION = round((time.time()-START_TIME)/60, 1)\n",
        "print(f\"> In {(DURATION)} minutes\")\n",
        "\n",
        "github_link = 'https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition/tree/master/RUNS/'+str(CURRENT_RUN)\n",
        "github_link = github_link.replace(\" \", \"%20\")\n",
        "hyperparameters_string = get_hyperparameter_string(lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, multilrstep=DO_MULTILR_STEP_DOWN, gamma=GAMMA)\n",
        "if DUMP_FINAL_RESULTS_ON_GSPREADSHEET:\n",
        "  dump_on_gspreadsheet(CURRENT_RUN, GITHUB_USER, github_link, METHOD, RANDOM_STATE, False, True, CIFAR_NORMALIZE, group_losses_train, group_losses_eval, group_accuracies_train, group_accuracies_eval, group_accuracies_eval_curr, DURATION, use_validation=USE_VALIDATION_SET, lwf = True, hyperparameters=hyperparameters_string)\n",
        "\n",
        "\t\n",
        "beep()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************************\n",
            "NEW GROUP OF CLASSES 1°/10\n",
            "Training set length: 5000\n",
            "Test set length: 1000\n",
            "Starting epoch 1/70, LR = [2.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- Initial loss on train: 0.8017435073852539\n",
            "--- Epoch 1, Loss on train: 0.027783997356891632\n",
            "Starting epoch 2/70, LR = [2.0]\n",
            "--- Epoch 2, Loss on train: 0.023977411910891533\n",
            "Starting epoch 3/70, LR = [2.0]\n",
            "--- Epoch 3, Loss on train: 0.024399656802415848\n",
            "Starting epoch 4/70, LR = [2.0]\n",
            "--- Epoch 4, Loss on train: 0.017919743433594704\n",
            "Starting epoch 5/70, LR = [2.0]\n",
            "--- Epoch 5, Loss on train: 0.019129332154989243\n",
            "Starting epoch 6/70, LR = [2.0]\n",
            "--- Epoch 6, Loss on train: 0.01837143860757351\n",
            "Starting epoch 7/70, LR = [2.0]\n",
            "--- Epoch 7, Loss on train: 0.019016530364751816\n",
            "Starting epoch 8/70, LR = [2.0]\n",
            "--- Epoch 8, Loss on train: 0.016413282603025436\n",
            "Starting epoch 9/70, LR = [2.0]\n",
            "--- Epoch 9, Loss on train: 0.0159798264503479\n",
            "Starting epoch 10/70, LR = [2.0]\n",
            "--- Epoch 10, Loss on train: 0.014840185642242432\n",
            "Starting epoch 11/70, LR = [2.0]\n",
            "--- Epoch 11, Loss on train: 0.013364257290959358\n",
            "Starting epoch 12/70, LR = [2.0]\n",
            "--- Epoch 12, Loss on train: 0.014030109159648418\n",
            "Starting epoch 13/70, LR = [2.0]\n",
            "--- Epoch 13, Loss on train: 0.014352093450725079\n",
            "Starting epoch 14/70, LR = [2.0]\n",
            "--- Epoch 14, Loss on train: 0.013869021087884903\n",
            "Starting epoch 15/70, LR = [2.0]\n",
            "--- Epoch 15, Loss on train: 0.014475895091891289\n",
            "Starting epoch 16/70, LR = [2.0]\n",
            "--- Epoch 16, Loss on train: 0.011596281081438065\n",
            "Starting epoch 17/70, LR = [2.0]\n",
            "--- Epoch 17, Loss on train: 0.009255743585526943\n",
            "Starting epoch 18/70, LR = [2.0]\n",
            "--- Epoch 18, Loss on train: 0.010363306850194931\n",
            "Starting epoch 19/70, LR = [2.0]\n",
            "--- Epoch 19, Loss on train: 0.012270855717360973\n",
            "Starting epoch 20/70, LR = [2.0]\n",
            "--- Epoch 20, Loss on train: 0.008079370483756065\n",
            "Starting epoch 21/70, LR = [2.0]\n",
            "--- Epoch 21, Loss on train: 0.011719925329089165\n",
            "Starting epoch 22/70, LR = [2.0]\n",
            "--- Epoch 22, Loss on train: 0.010823329910635948\n",
            "Starting epoch 23/70, LR = [2.0]\n",
            "--- Epoch 23, Loss on train: 0.010575425811111927\n",
            "Starting epoch 24/70, LR = [2.0]\n",
            "--- Epoch 24, Loss on train: 0.009878711774945259\n",
            "Starting epoch 25/70, LR = [2.0]\n",
            "--- Epoch 25, Loss on train: 0.008223466575145721\n",
            "Starting epoch 26/70, LR = [2.0]\n",
            "--- Epoch 26, Loss on train: 0.00978097040206194\n",
            "Starting epoch 27/70, LR = [2.0]\n",
            "--- Epoch 27, Loss on train: 0.00996125303208828\n",
            "Starting epoch 28/70, LR = [2.0]\n",
            "--- Epoch 28, Loss on train: 0.01003172155469656\n",
            "Starting epoch 29/70, LR = [2.0]\n",
            "--- Epoch 29, Loss on train: 0.009786335751414299\n",
            "Starting epoch 30/70, LR = [2.0]\n",
            "--- Epoch 30, Loss on train: 0.009378905408084393\n",
            "Starting epoch 31/70, LR = [2.0]\n",
            "--- Epoch 31, Loss on train: 0.007715997751802206\n",
            "Starting epoch 32/70, LR = [2.0]\n",
            "--- Epoch 32, Loss on train: 0.009076807647943497\n",
            "Starting epoch 33/70, LR = [2.0]\n",
            "--- Epoch 33, Loss on train: 0.00718196714296937\n",
            "Starting epoch 34/70, LR = [2.0]\n",
            "--- Epoch 34, Loss on train: 0.007026170380413532\n",
            "Starting epoch 35/70, LR = [2.0]\n",
            "--- Epoch 35, Loss on train: 0.008325585164129734\n",
            "Starting epoch 36/70, LR = [2.0]\n",
            "--- Epoch 36, Loss on train: 0.005949557758867741\n",
            "Starting epoch 37/70, LR = [2.0]\n",
            "--- Epoch 37, Loss on train: 0.00570896128192544\n",
            "Starting epoch 38/70, LR = [2.0]\n",
            "--- Epoch 38, Loss on train: 0.0066234637051820755\n",
            "Starting epoch 39/70, LR = [2.0]\n",
            "--- Epoch 39, Loss on train: 0.00903630256652832\n",
            "Starting epoch 40/70, LR = [2.0]\n",
            "--- Epoch 40, Loss on train: 0.006891993340104818\n",
            "Starting epoch 41/70, LR = [2.0]\n",
            "--- Epoch 41, Loss on train: 0.006462461780756712\n",
            "Starting epoch 42/70, LR = [2.0]\n",
            "--- Epoch 42, Loss on train: 0.007600855547934771\n",
            "Starting epoch 43/70, LR = [2.0]\n",
            "--- Epoch 43, Loss on train: 0.0065126740373671055\n",
            "Starting epoch 44/70, LR = [2.0]\n",
            "--- Epoch 44, Loss on train: 0.005630968604236841\n",
            "Starting epoch 45/70, LR = [2.0]\n",
            "--- Epoch 45, Loss on train: 0.005620460957288742\n",
            "Starting epoch 46/70, LR = [2.0]\n",
            "--- Epoch 46, Loss on train: 0.006195280235260725\n",
            "Starting epoch 47/70, LR = [2.0]\n",
            "--- Epoch 47, Loss on train: 0.0048856595531105995\n",
            "Starting epoch 48/70, LR = [2.0]\n",
            "--- Epoch 48, Loss on train: 0.004223942756652832\n",
            "Starting epoch 49/70, LR = [2.0]\n",
            "--- Epoch 49, Loss on train: 0.005443244241178036\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.002710689790546894\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.0015841483836993575\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.003142709843814373\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.0027069980278611183\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.0029419546481221914\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.0024109710939228535\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.0031690888572484255\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.0026341367047280073\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.0016789278015494347\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.0017809735145419836\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.00140911724884063\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.0015444911550730467\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.001651375088840723\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.0019275808008387685\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.001577346003614366\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.0008307413663715124\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.0013762337621301413\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.0013099053176119924\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.001266800332814455\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.0012723536929115653\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.0009565218351781368\n",
            "Length on train dataset: 50000\n",
            "Loss on eval (group): 0.0\n",
            "Accuracy on eval (group): 0.836\n",
            "Accuracy on train (group): 0.9938\n",
            "Accuracy on test novel classes (group): 0.836\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 2°/10\n",
            "Training set length: 5000\n",
            "Test set length: 2000\n",
            "Starting epoch 1/70, LR = [2.0]\n",
            "--- Initial loss on train: 0.12434960901737213\n",
            "--- Epoch 1, Loss on train: 0.034362372010946274\n",
            "Starting epoch 2/70, LR = [2.0]\n",
            "--- Epoch 2, Loss on train: 0.02786298468708992\n",
            "Starting epoch 3/70, LR = [2.0]\n",
            "--- Epoch 3, Loss on train: 0.026677219197154045\n",
            "Starting epoch 4/70, LR = [2.0]\n",
            "--- Epoch 4, Loss on train: 0.022687392309308052\n",
            "Starting epoch 5/70, LR = [2.0]\n",
            "--- Epoch 5, Loss on train: 0.021259350702166557\n",
            "Starting epoch 6/70, LR = [2.0]\n",
            "--- Epoch 6, Loss on train: 0.022220158949494362\n",
            "Starting epoch 7/70, LR = [2.0]\n",
            "--- Epoch 7, Loss on train: 0.02360670454800129\n",
            "Starting epoch 8/70, LR = [2.0]\n",
            "--- Epoch 8, Loss on train: 0.02117314375936985\n",
            "Starting epoch 9/70, LR = [2.0]\n",
            "--- Epoch 9, Loss on train: 0.02094944939017296\n",
            "Starting epoch 10/70, LR = [2.0]\n",
            "--- Epoch 10, Loss on train: 0.024384643882513046\n",
            "Starting epoch 11/70, LR = [2.0]\n",
            "--- Epoch 11, Loss on train: 0.02323746122419834\n",
            "Starting epoch 12/70, LR = [2.0]\n",
            "--- Epoch 12, Loss on train: 0.02155701071023941\n",
            "Starting epoch 13/70, LR = [2.0]\n",
            "--- Epoch 13, Loss on train: 0.019747082144021988\n",
            "Starting epoch 14/70, LR = [2.0]\n",
            "--- Epoch 14, Loss on train: 0.02185743674635887\n",
            "Starting epoch 15/70, LR = [2.0]\n",
            "--- Epoch 15, Loss on train: 0.019228143617510796\n",
            "Starting epoch 16/70, LR = [2.0]\n",
            "--- Epoch 16, Loss on train: 0.018587931990623474\n",
            "Starting epoch 17/70, LR = [2.0]\n",
            "--- Epoch 17, Loss on train: 0.022307302802801132\n",
            "Starting epoch 18/70, LR = [2.0]\n",
            "--- Epoch 18, Loss on train: 0.019647393375635147\n",
            "Starting epoch 19/70, LR = [2.0]\n",
            "--- Epoch 19, Loss on train: 0.017804715782403946\n",
            "Starting epoch 20/70, LR = [2.0]\n",
            "--- Epoch 20, Loss on train: 0.01990283839404583\n",
            "Starting epoch 21/70, LR = [2.0]\n",
            "--- Epoch 21, Loss on train: 0.0191042497754097\n",
            "Starting epoch 22/70, LR = [2.0]\n",
            "--- Epoch 22, Loss on train: 0.018279939889907837\n",
            "Starting epoch 23/70, LR = [2.0]\n",
            "--- Epoch 23, Loss on train: 0.015726959332823753\n",
            "Starting epoch 24/70, LR = [2.0]\n",
            "--- Epoch 24, Loss on train: 0.020943691954016685\n",
            "Starting epoch 25/70, LR = [2.0]\n",
            "--- Epoch 25, Loss on train: 0.016525570303201675\n",
            "Starting epoch 26/70, LR = [2.0]\n",
            "--- Epoch 26, Loss on train: 0.019957447424530983\n",
            "Starting epoch 27/70, LR = [2.0]\n",
            "--- Epoch 27, Loss on train: 0.018334228545427322\n",
            "Starting epoch 28/70, LR = [2.0]\n",
            "--- Epoch 28, Loss on train: 0.019349178299307823\n",
            "Starting epoch 29/70, LR = [2.0]\n",
            "--- Epoch 29, Loss on train: 0.018208026885986328\n",
            "Starting epoch 30/70, LR = [2.0]\n",
            "--- Epoch 30, Loss on train: 0.016862250864505768\n",
            "Starting epoch 31/70, LR = [2.0]\n",
            "--- Epoch 31, Loss on train: 0.016588104888796806\n",
            "Starting epoch 32/70, LR = [2.0]\n",
            "--- Epoch 32, Loss on train: 0.016269857063889503\n",
            "Starting epoch 33/70, LR = [2.0]\n",
            "--- Epoch 33, Loss on train: 0.017168764024972916\n",
            "Starting epoch 34/70, LR = [2.0]\n",
            "--- Epoch 34, Loss on train: 0.016823384910821915\n",
            "Starting epoch 35/70, LR = [2.0]\n",
            "--- Epoch 35, Loss on train: 0.015982121229171753\n",
            "Starting epoch 36/70, LR = [2.0]\n",
            "--- Epoch 36, Loss on train: 0.017814040184020996\n",
            "Starting epoch 37/70, LR = [2.0]\n",
            "--- Epoch 37, Loss on train: 0.017095347866415977\n",
            "Starting epoch 38/70, LR = [2.0]\n",
            "--- Epoch 38, Loss on train: 0.01863991841673851\n",
            "Starting epoch 39/70, LR = [2.0]\n",
            "--- Epoch 39, Loss on train: 0.01533115841448307\n",
            "Starting epoch 40/70, LR = [2.0]\n",
            "--- Epoch 40, Loss on train: 0.01619862951338291\n",
            "Starting epoch 41/70, LR = [2.0]\n",
            "--- Epoch 41, Loss on train: 0.01583264209330082\n",
            "Starting epoch 42/70, LR = [2.0]\n",
            "--- Epoch 42, Loss on train: 0.017526475712656975\n",
            "Starting epoch 43/70, LR = [2.0]\n",
            "--- Epoch 43, Loss on train: 0.017162984237074852\n",
            "Starting epoch 44/70, LR = [2.0]\n",
            "--- Epoch 44, Loss on train: 0.014714173041284084\n",
            "Starting epoch 45/70, LR = [2.0]\n",
            "--- Epoch 45, Loss on train: 0.017536384984850883\n",
            "Starting epoch 46/70, LR = [2.0]\n",
            "--- Epoch 46, Loss on train: 0.015910256654024124\n",
            "Starting epoch 47/70, LR = [2.0]\n",
            "--- Epoch 47, Loss on train: 0.013439704664051533\n",
            "Starting epoch 48/70, LR = [2.0]\n",
            "--- Epoch 48, Loss on train: 0.017567498609423637\n",
            "Starting epoch 49/70, LR = [2.0]\n",
            "--- Epoch 49, Loss on train: 0.014830796048045158\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.013921667821705341\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.014815246686339378\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.012795371934771538\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.011950102634727955\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.01165652833878994\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.01065971702337265\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.010859946720302105\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.013009472750127316\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.01180089171975851\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.012141234241425991\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.012343215756118298\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.012426388449966908\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.011611109599471092\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.010995605029165745\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.011368326842784882\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.011603457853198051\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.010015701875090599\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.011460539884865284\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.011700205504894257\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.01195560209453106\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.01209005806595087\n",
            "Length on train dataset: 50000\n",
            "Loss on eval (group): 0.0\n",
            "Accuracy on eval (group): 0.649\n",
            "Accuracy on train (group): 0.9954\n",
            "Accuracy on test novel classes (group): 0.866\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 3°/10\n",
            "Training set length: 5000\n",
            "Test set length: 3000\n",
            "Starting epoch 1/70, LR = [2.0]\n",
            "--- Initial loss on train: 0.14769713580608368\n",
            "--- Epoch 1, Loss on train: 0.054246723651885986\n",
            "Starting epoch 2/70, LR = [2.0]\n",
            "--- Epoch 2, Loss on train: 0.04606284946203232\n",
            "Starting epoch 3/70, LR = [2.0]\n",
            "--- Epoch 3, Loss on train: 0.04376187548041344\n",
            "Starting epoch 4/70, LR = [2.0]\n",
            "--- Epoch 4, Loss on train: 0.042640216648578644\n",
            "Starting epoch 5/70, LR = [2.0]\n",
            "--- Epoch 5, Loss on train: 0.04182325303554535\n",
            "Starting epoch 6/70, LR = [2.0]\n",
            "--- Epoch 6, Loss on train: 0.037487003952264786\n",
            "Starting epoch 7/70, LR = [2.0]\n",
            "--- Epoch 7, Loss on train: 0.04047686606645584\n",
            "Starting epoch 8/70, LR = [2.0]\n",
            "--- Epoch 8, Loss on train: 0.041474368423223495\n",
            "Starting epoch 9/70, LR = [2.0]\n",
            "--- Epoch 9, Loss on train: 0.039488814771175385\n",
            "Starting epoch 10/70, LR = [2.0]\n",
            "--- Epoch 10, Loss on train: 0.03935600817203522\n",
            "Starting epoch 11/70, LR = [2.0]\n",
            "--- Epoch 11, Loss on train: 0.03586634621024132\n",
            "Starting epoch 12/70, LR = [2.0]\n",
            "--- Epoch 12, Loss on train: 0.03973286598920822\n",
            "Starting epoch 13/70, LR = [2.0]\n",
            "--- Epoch 13, Loss on train: 0.03780476003885269\n",
            "Starting epoch 14/70, LR = [2.0]\n",
            "--- Epoch 14, Loss on train: 0.03468868136405945\n",
            "Starting epoch 15/70, LR = [2.0]\n",
            "--- Epoch 15, Loss on train: 0.03531019762158394\n",
            "Starting epoch 16/70, LR = [2.0]\n",
            "--- Epoch 16, Loss on train: 0.037380654364824295\n",
            "Starting epoch 17/70, LR = [2.0]\n",
            "--- Epoch 17, Loss on train: 0.03731809929013252\n",
            "Starting epoch 18/70, LR = [2.0]\n",
            "--- Epoch 18, Loss on train: 0.03562396764755249\n",
            "Starting epoch 19/70, LR = [2.0]\n",
            "--- Epoch 19, Loss on train: 0.03556383028626442\n",
            "Starting epoch 20/70, LR = [2.0]\n",
            "--- Epoch 20, Loss on train: 0.03700924292206764\n",
            "Starting epoch 21/70, LR = [2.0]\n",
            "--- Epoch 21, Loss on train: 0.03716205433011055\n",
            "Starting epoch 22/70, LR = [2.0]\n",
            "--- Epoch 22, Loss on train: 0.03407398238778114\n",
            "Starting epoch 23/70, LR = [2.0]\n",
            "--- Epoch 23, Loss on train: 0.03631120175123215\n",
            "Starting epoch 24/70, LR = [2.0]\n",
            "--- Epoch 24, Loss on train: 0.03417475149035454\n",
            "Starting epoch 25/70, LR = [2.0]\n",
            "--- Epoch 25, Loss on train: 0.03445782512426376\n",
            "Starting epoch 26/70, LR = [2.0]\n",
            "--- Epoch 26, Loss on train: 0.033923082053661346\n",
            "Starting epoch 27/70, LR = [2.0]\n",
            "--- Epoch 27, Loss on train: 0.03344498947262764\n",
            "Starting epoch 28/70, LR = [2.0]\n",
            "--- Epoch 28, Loss on train: 0.033764202147722244\n",
            "Starting epoch 29/70, LR = [2.0]\n",
            "--- Epoch 29, Loss on train: 0.0356135331094265\n",
            "Starting epoch 30/70, LR = [2.0]\n",
            "--- Epoch 30, Loss on train: 0.03399943187832832\n",
            "Starting epoch 31/70, LR = [2.0]\n",
            "--- Epoch 31, Loss on train: 0.032242387533187866\n",
            "Starting epoch 32/70, LR = [2.0]\n",
            "--- Epoch 32, Loss on train: 0.03236851468682289\n",
            "Starting epoch 33/70, LR = [2.0]\n",
            "--- Epoch 33, Loss on train: 0.03335868939757347\n",
            "Starting epoch 34/70, LR = [2.0]\n",
            "--- Epoch 34, Loss on train: 0.03279980644583702\n",
            "Starting epoch 35/70, LR = [2.0]\n",
            "--- Epoch 35, Loss on train: 0.03450365364551544\n",
            "Starting epoch 36/70, LR = [2.0]\n",
            "--- Epoch 36, Loss on train: 0.03355494141578674\n",
            "Starting epoch 37/70, LR = [2.0]\n",
            "--- Epoch 37, Loss on train: 0.0330004058778286\n",
            "Starting epoch 38/70, LR = [2.0]\n",
            "--- Epoch 38, Loss on train: 0.032696619629859924\n",
            "Starting epoch 39/70, LR = [2.0]\n",
            "--- Epoch 39, Loss on train: 0.03155631944537163\n",
            "Starting epoch 40/70, LR = [2.0]\n",
            "--- Epoch 40, Loss on train: 0.03191341459751129\n",
            "Starting epoch 41/70, LR = [2.0]\n",
            "--- Epoch 41, Loss on train: 0.03438708558678627\n",
            "Starting epoch 42/70, LR = [2.0]\n",
            "--- Epoch 42, Loss on train: 0.03215569257736206\n",
            "Starting epoch 43/70, LR = [2.0]\n",
            "--- Epoch 43, Loss on train: 0.03256330266594887\n",
            "Starting epoch 44/70, LR = [2.0]\n",
            "--- Epoch 44, Loss on train: 0.033380549401044846\n",
            "Starting epoch 45/70, LR = [2.0]\n",
            "--- Epoch 45, Loss on train: 0.03433986008167267\n",
            "Starting epoch 46/70, LR = [2.0]\n",
            "--- Epoch 46, Loss on train: 0.032325319945812225\n",
            "Starting epoch 47/70, LR = [2.0]\n",
            "--- Epoch 47, Loss on train: 0.030630001798272133\n",
            "Starting epoch 48/70, LR = [2.0]\n",
            "--- Epoch 48, Loss on train: 0.032659564167261124\n",
            "Starting epoch 49/70, LR = [2.0]\n",
            "--- Epoch 49, Loss on train: 0.030706537887454033\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.029716653749346733\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.028032125905156136\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.028101395815610886\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.028705745935440063\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.028145750984549522\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.026496324688196182\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.027815641835331917\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.02748550847172737\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.027379998937249184\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.028689755126833916\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.027996176853775978\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.026389718055725098\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.026485174894332886\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.028388291597366333\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.02634476311504841\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.02789369784295559\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.027506636455655098\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.027581514790654182\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.02807907573878765\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.02648632414638996\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.027300357818603516\n",
            "Length on train dataset: 50000\n",
            "Loss on eval (group): 0.0\n",
            "Accuracy on eval (group): 0.5886666666666667\n",
            "Accuracy on train (group): 0.9994\n",
            "Accuracy on test novel classes (group): 0.881\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 4°/10\n",
            "Training set length: 5000\n",
            "Test set length: 4000\n",
            "Starting epoch 1/70, LR = [2.0]\n",
            "--- Initial loss on train: 0.15473024547100067\n",
            "--- Epoch 1, Loss on train: 0.06567155569791794\n",
            "Starting epoch 2/70, LR = [2.0]\n",
            "--- Epoch 2, Loss on train: 0.06120428070425987\n",
            "Starting epoch 3/70, LR = [2.0]\n",
            "--- Epoch 3, Loss on train: 0.05942006781697273\n",
            "Starting epoch 4/70, LR = [2.0]\n",
            "--- Epoch 4, Loss on train: 0.05715244635939598\n",
            "Starting epoch 5/70, LR = [2.0]\n",
            "--- Epoch 5, Loss on train: 0.05584125220775604\n",
            "Starting epoch 6/70, LR = [2.0]\n",
            "--- Epoch 6, Loss on train: 0.05839165672659874\n",
            "Starting epoch 7/70, LR = [2.0]\n",
            "--- Epoch 7, Loss on train: 0.05357420817017555\n",
            "Starting epoch 8/70, LR = [2.0]\n",
            "--- Epoch 8, Loss on train: 0.05741924047470093\n",
            "Starting epoch 9/70, LR = [2.0]\n",
            "--- Epoch 9, Loss on train: 0.055785853415727615\n",
            "Starting epoch 10/70, LR = [2.0]\n",
            "--- Epoch 10, Loss on train: 0.05721381679177284\n",
            "Starting epoch 11/70, LR = [2.0]\n",
            "--- Epoch 11, Loss on train: 0.054141510277986526\n",
            "Starting epoch 12/70, LR = [2.0]\n",
            "--- Epoch 12, Loss on train: 0.055429667234420776\n",
            "Starting epoch 13/70, LR = [2.0]\n",
            "--- Epoch 13, Loss on train: 0.056016966700553894\n",
            "Starting epoch 14/70, LR = [2.0]\n",
            "--- Epoch 14, Loss on train: 0.0521291047334671\n",
            "Starting epoch 15/70, LR = [2.0]\n",
            "--- Epoch 15, Loss on train: 0.05630258843302727\n",
            "Starting epoch 16/70, LR = [2.0]\n",
            "--- Epoch 16, Loss on train: 0.055634506046772\n",
            "Starting epoch 17/70, LR = [2.0]\n",
            "--- Epoch 17, Loss on train: 0.05721117928624153\n",
            "Starting epoch 18/70, LR = [2.0]\n",
            "--- Epoch 18, Loss on train: 0.0510031133890152\n",
            "Starting epoch 19/70, LR = [2.0]\n",
            "--- Epoch 19, Loss on train: 0.05231758579611778\n",
            "Starting epoch 20/70, LR = [2.0]\n",
            "--- Epoch 20, Loss on train: 0.0526295080780983\n",
            "Starting epoch 21/70, LR = [2.0]\n",
            "--- Epoch 21, Loss on train: 0.05310547724366188\n",
            "Starting epoch 22/70, LR = [2.0]\n",
            "--- Epoch 22, Loss on train: 0.05397259443998337\n",
            "Starting epoch 23/70, LR = [2.0]\n",
            "--- Epoch 23, Loss on train: 0.05457104742527008\n",
            "Starting epoch 24/70, LR = [2.0]\n",
            "--- Epoch 24, Loss on train: 0.05367018282413483\n",
            "Starting epoch 25/70, LR = [2.0]\n",
            "--- Epoch 25, Loss on train: 0.05189264193177223\n",
            "Starting epoch 26/70, LR = [2.0]\n",
            "--- Epoch 26, Loss on train: 0.05138331279158592\n",
            "Starting epoch 27/70, LR = [2.0]\n",
            "--- Epoch 27, Loss on train: 0.05196473002433777\n",
            "Starting epoch 28/70, LR = [2.0]\n",
            "--- Epoch 28, Loss on train: 0.04996631667017937\n",
            "Starting epoch 29/70, LR = [2.0]\n",
            "--- Epoch 29, Loss on train: 0.05331188067793846\n",
            "Starting epoch 30/70, LR = [2.0]\n",
            "--- Epoch 30, Loss on train: 0.05166412144899368\n",
            "Starting epoch 31/70, LR = [2.0]\n",
            "--- Epoch 31, Loss on train: 0.05198682099580765\n",
            "Starting epoch 32/70, LR = [2.0]\n",
            "--- Epoch 32, Loss on train: 0.04945434629917145\n",
            "Starting epoch 33/70, LR = [2.0]\n",
            "--- Epoch 33, Loss on train: 0.05028815194964409\n",
            "Starting epoch 34/70, LR = [2.0]\n",
            "--- Epoch 34, Loss on train: 0.050764620304107666\n",
            "Starting epoch 35/70, LR = [2.0]\n",
            "--- Epoch 35, Loss on train: 0.04938909411430359\n",
            "Starting epoch 36/70, LR = [2.0]\n",
            "--- Epoch 36, Loss on train: 0.05111626535654068\n",
            "Starting epoch 37/70, LR = [2.0]\n",
            "--- Epoch 37, Loss on train: 0.050901416689157486\n",
            "Starting epoch 38/70, LR = [2.0]\n",
            "--- Epoch 38, Loss on train: 0.046714238822460175\n",
            "Starting epoch 39/70, LR = [2.0]\n",
            "--- Epoch 39, Loss on train: 0.04989229142665863\n",
            "Starting epoch 40/70, LR = [2.0]\n",
            "--- Epoch 40, Loss on train: 0.04801775515079498\n",
            "Starting epoch 41/70, LR = [2.0]\n",
            "--- Epoch 41, Loss on train: 0.04846227914094925\n",
            "Starting epoch 42/70, LR = [2.0]\n",
            "--- Epoch 42, Loss on train: 0.050138313323259354\n",
            "Starting epoch 43/70, LR = [2.0]\n",
            "--- Epoch 43, Loss on train: 0.048876743763685226\n",
            "Starting epoch 44/70, LR = [2.0]\n",
            "--- Epoch 44, Loss on train: 0.04845869913697243\n",
            "Starting epoch 45/70, LR = [2.0]\n",
            "--- Epoch 45, Loss on train: 0.051784638315439224\n",
            "Starting epoch 46/70, LR = [2.0]\n",
            "--- Epoch 46, Loss on train: 0.04860282689332962\n",
            "Starting epoch 47/70, LR = [2.0]\n",
            "--- Epoch 47, Loss on train: 0.049667615443468094\n",
            "Starting epoch 48/70, LR = [2.0]\n",
            "--- Epoch 48, Loss on train: 0.04958159476518631\n",
            "Starting epoch 49/70, LR = [2.0]\n",
            "--- Epoch 49, Loss on train: 0.049006927758455276\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.04445991292595863\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.04346396401524544\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.04414438083767891\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.04194473475217819\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.04266201704740524\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.04358157142996788\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.04363695904612541\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.04220197722315788\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.0425461083650589\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.04269209876656532\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.04270093888044357\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.043841276317834854\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.042786598205566406\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.04183591902256012\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.04177599400281906\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.04188340902328491\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.04274895414710045\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.041325148195028305\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.04143359884619713\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.04311757907271385\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.04177902638912201\n",
            "Length on train dataset: 50000\n",
            "Loss on eval (group): 0.0\n",
            "Accuracy on eval (group): 0.47175\n",
            "Accuracy on train (group): 0.9966\n",
            "Accuracy on test novel classes (group): 0.795\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 5°/10\n",
            "Training set length: 5000\n",
            "Test set length: 5000\n",
            "Starting epoch 1/70, LR = [2.0]\n",
            "--- Initial loss on train: 0.16870024800300598\n",
            "--- Epoch 1, Loss on train: 0.08313827216625214\n",
            "Starting epoch 2/70, LR = [2.0]\n",
            "--- Epoch 2, Loss on train: 0.0787239596247673\n",
            "Starting epoch 3/70, LR = [2.0]\n",
            "--- Epoch 3, Loss on train: 0.07613053917884827\n",
            "Starting epoch 4/70, LR = [2.0]\n",
            "--- Epoch 4, Loss on train: 0.07702617347240448\n",
            "Starting epoch 5/70, LR = [2.0]\n",
            "--- Epoch 5, Loss on train: 0.07662369310855865\n",
            "Starting epoch 6/70, LR = [2.0]\n",
            "--- Epoch 6, Loss on train: 0.07402723282575607\n",
            "Starting epoch 7/70, LR = [2.0]\n",
            "--- Epoch 7, Loss on train: 0.06983145326375961\n",
            "Starting epoch 8/70, LR = [2.0]\n",
            "--- Epoch 8, Loss on train: 0.07344668358564377\n",
            "Starting epoch 9/70, LR = [2.0]\n",
            "--- Epoch 9, Loss on train: 0.07137032598257065\n",
            "Starting epoch 10/70, LR = [2.0]\n",
            "--- Epoch 10, Loss on train: 0.07063598185777664\n",
            "Starting epoch 11/70, LR = [2.0]\n",
            "--- Epoch 11, Loss on train: 0.07187020778656006\n",
            "Starting epoch 12/70, LR = [2.0]\n",
            "--- Epoch 12, Loss on train: 0.07274866104125977\n",
            "Starting epoch 13/70, LR = [2.0]\n",
            "--- Epoch 13, Loss on train: 0.07196351885795593\n",
            "Starting epoch 14/70, LR = [2.0]\n",
            "--- Epoch 14, Loss on train: 0.07219478487968445\n",
            "Starting epoch 15/70, LR = [2.0]\n",
            "--- Epoch 15, Loss on train: 0.06998233497142792\n",
            "Starting epoch 16/70, LR = [2.0]\n",
            "--- Epoch 16, Loss on train: 0.0692366287112236\n",
            "Starting epoch 17/70, LR = [2.0]\n",
            "--- Epoch 17, Loss on train: 0.0708102434873581\n",
            "Starting epoch 18/70, LR = [2.0]\n",
            "--- Epoch 18, Loss on train: 0.06925103813409805\n",
            "Starting epoch 19/70, LR = [2.0]\n",
            "--- Epoch 19, Loss on train: 0.07080889493227005\n",
            "Starting epoch 20/70, LR = [2.0]\n",
            "--- Epoch 20, Loss on train: 0.06989745795726776\n",
            "Starting epoch 21/70, LR = [2.0]\n",
            "--- Epoch 21, Loss on train: 0.06481164693832397\n",
            "Starting epoch 22/70, LR = [2.0]\n",
            "--- Epoch 22, Loss on train: 0.0679578110575676\n",
            "Starting epoch 23/70, LR = [2.0]\n",
            "--- Epoch 23, Loss on train: 0.06891673058271408\n",
            "Starting epoch 24/70, LR = [2.0]\n",
            "--- Epoch 24, Loss on train: 0.06861802190542221\n",
            "Starting epoch 25/70, LR = [2.0]\n",
            "--- Epoch 25, Loss on train: 0.06560967117547989\n",
            "Starting epoch 26/70, LR = [2.0]\n",
            "--- Epoch 26, Loss on train: 0.0662180483341217\n",
            "Starting epoch 27/70, LR = [2.0]\n",
            "--- Epoch 27, Loss on train: 0.07119464129209518\n",
            "Starting epoch 28/70, LR = [2.0]\n",
            "--- Epoch 28, Loss on train: 0.0681651383638382\n",
            "Starting epoch 29/70, LR = [2.0]\n",
            "--- Epoch 29, Loss on train: 0.06711259484291077\n",
            "Starting epoch 30/70, LR = [2.0]\n",
            "--- Epoch 30, Loss on train: 0.07124608010053635\n",
            "Starting epoch 31/70, LR = [2.0]\n",
            "--- Epoch 31, Loss on train: 0.06846468150615692\n",
            "Starting epoch 32/70, LR = [2.0]\n",
            "--- Epoch 32, Loss on train: 0.0649615228176117\n",
            "Starting epoch 33/70, LR = [2.0]\n",
            "--- Epoch 33, Loss on train: 0.06588802486658096\n",
            "Starting epoch 34/70, LR = [2.0]\n",
            "--- Epoch 34, Loss on train: 0.06467776745557785\n",
            "Starting epoch 35/70, LR = [2.0]\n",
            "--- Epoch 35, Loss on train: 0.06832285970449448\n",
            "Starting epoch 36/70, LR = [2.0]\n",
            "--- Epoch 36, Loss on train: 0.06834759563207626\n",
            "Starting epoch 37/70, LR = [2.0]\n",
            "--- Epoch 37, Loss on train: 0.06494157761335373\n",
            "Starting epoch 38/70, LR = [2.0]\n",
            "--- Epoch 38, Loss on train: 0.06589541584253311\n",
            "Starting epoch 39/70, LR = [2.0]\n",
            "--- Epoch 39, Loss on train: 0.0655052661895752\n",
            "Starting epoch 40/70, LR = [2.0]\n",
            "--- Epoch 40, Loss on train: 0.06538089364767075\n",
            "Starting epoch 41/70, LR = [2.0]\n",
            "--- Epoch 41, Loss on train: 0.06663135439157486\n",
            "Starting epoch 42/70, LR = [2.0]\n",
            "--- Epoch 42, Loss on train: 0.06511970609426498\n",
            "Starting epoch 43/70, LR = [2.0]\n",
            "--- Epoch 43, Loss on train: 0.06521826982498169\n",
            "Starting epoch 44/70, LR = [2.0]\n",
            "--- Epoch 44, Loss on train: 0.06452053040266037\n",
            "Starting epoch 45/70, LR = [2.0]\n",
            "--- Epoch 45, Loss on train: 0.0661141499876976\n",
            "Starting epoch 46/70, LR = [2.0]\n",
            "--- Epoch 46, Loss on train: 0.06938982009887695\n",
            "Starting epoch 47/70, LR = [2.0]\n",
            "--- Epoch 47, Loss on train: 0.0632057636976242\n",
            "Starting epoch 48/70, LR = [2.0]\n",
            "--- Epoch 48, Loss on train: 0.06553161144256592\n",
            "Starting epoch 49/70, LR = [2.0]\n",
            "--- Epoch 49, Loss on train: 0.06776167452335358\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.06057130545377731\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.06232032552361488\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.0610385537147522\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.05986766144633293\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.06168398633599281\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.06099410727620125\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.06050972640514374\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.06250929087400436\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.05645548552274704\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.059427253901958466\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.05952407792210579\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.0617205947637558\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.05763791874051094\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.05863247811794281\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.05868874490261078\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.058631207793951035\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.05831780284643173\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.05852437764406204\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.057330816984176636\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.05782134830951691\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.05607542768120766\n",
            "Length on train dataset: 50000\n",
            "Loss on eval (group): 0.0\n",
            "Accuracy on eval (group): 0.379\n",
            "Accuracy on train (group): 0.9974\n",
            "Accuracy on test novel classes (group): 0.77\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 6°/10\n",
            "Training set length: 5000\n",
            "Test set length: 6000\n",
            "Starting epoch 1/70, LR = [2.0]\n",
            "--- Initial loss on train: 0.20418421924114227\n",
            "--- Epoch 1, Loss on train: 0.09735188633203506\n",
            "Starting epoch 2/70, LR = [2.0]\n",
            "--- Epoch 2, Loss on train: 0.09721170365810394\n",
            "Starting epoch 3/70, LR = [2.0]\n",
            "--- Epoch 3, Loss on train: 0.09336967021226883\n",
            "Starting epoch 4/70, LR = [2.0]\n",
            "--- Epoch 4, Loss on train: 0.0866178572177887\n",
            "Starting epoch 5/70, LR = [2.0]\n",
            "--- Epoch 5, Loss on train: 0.09491483122110367\n",
            "Starting epoch 6/70, LR = [2.0]\n",
            "--- Epoch 6, Loss on train: 0.08782712370157242\n",
            "Starting epoch 7/70, LR = [2.0]\n",
            "--- Epoch 7, Loss on train: 0.0871003121137619\n",
            "Starting epoch 8/70, LR = [2.0]\n",
            "--- Epoch 8, Loss on train: 0.09157509356737137\n",
            "Starting epoch 9/70, LR = [2.0]\n",
            "--- Epoch 9, Loss on train: 0.08981895446777344\n",
            "Starting epoch 10/70, LR = [2.0]\n",
            "--- Epoch 10, Loss on train: 0.09011129289865494\n",
            "Starting epoch 11/70, LR = [2.0]\n",
            "--- Epoch 11, Loss on train: 0.09093235433101654\n",
            "Starting epoch 12/70, LR = [2.0]\n",
            "--- Epoch 12, Loss on train: 0.08683396875858307\n",
            "Starting epoch 13/70, LR = [2.0]\n",
            "--- Epoch 13, Loss on train: 0.08545277267694473\n",
            "Starting epoch 14/70, LR = [2.0]\n",
            "--- Epoch 14, Loss on train: 0.08755330741405487\n",
            "Starting epoch 15/70, LR = [2.0]\n",
            "--- Epoch 15, Loss on train: 0.0873781368136406\n",
            "Starting epoch 16/70, LR = [2.0]\n",
            "--- Epoch 16, Loss on train: 0.0883740708231926\n",
            "Starting epoch 17/70, LR = [2.0]\n",
            "--- Epoch 17, Loss on train: 0.08698371797800064\n",
            "Starting epoch 18/70, LR = [2.0]\n",
            "--- Epoch 18, Loss on train: 0.08708532154560089\n",
            "Starting epoch 19/70, LR = [2.0]\n",
            "--- Epoch 19, Loss on train: 0.08653485029935837\n",
            "Starting epoch 20/70, LR = [2.0]\n",
            "--- Epoch 20, Loss on train: 0.0823272317647934\n",
            "Starting epoch 21/70, LR = [2.0]\n",
            "--- Epoch 21, Loss on train: 0.08764610439538956\n",
            "Starting epoch 22/70, LR = [2.0]\n",
            "--- Epoch 22, Loss on train: 0.0864873081445694\n",
            "Starting epoch 23/70, LR = [2.0]\n",
            "--- Epoch 23, Loss on train: 0.08555855602025986\n",
            "Starting epoch 24/70, LR = [2.0]\n",
            "--- Epoch 24, Loss on train: 0.08506905287504196\n",
            "Starting epoch 25/70, LR = [2.0]\n",
            "--- Epoch 25, Loss on train: 0.08709479868412018\n",
            "Starting epoch 26/70, LR = [2.0]\n",
            "--- Epoch 26, Loss on train: 0.0848226249217987\n",
            "Starting epoch 27/70, LR = [2.0]\n",
            "--- Epoch 27, Loss on train: 0.08652878552675247\n",
            "Starting epoch 28/70, LR = [2.0]\n",
            "--- Epoch 28, Loss on train: 0.08484084159135818\n",
            "Starting epoch 29/70, LR = [2.0]\n",
            "--- Epoch 29, Loss on train: 0.08655837923288345\n",
            "Starting epoch 30/70, LR = [2.0]\n",
            "--- Epoch 30, Loss on train: 0.08239627629518509\n",
            "Starting epoch 31/70, LR = [2.0]\n",
            "--- Epoch 31, Loss on train: 0.08702895045280457\n",
            "Starting epoch 32/70, LR = [2.0]\n",
            "--- Epoch 32, Loss on train: 0.08687327057123184\n",
            "Starting epoch 33/70, LR = [2.0]\n",
            "--- Epoch 33, Loss on train: 0.08459528535604477\n",
            "Starting epoch 34/70, LR = [2.0]\n",
            "--- Epoch 34, Loss on train: 0.08283203840255737\n",
            "Starting epoch 35/70, LR = [2.0]\n",
            "--- Epoch 35, Loss on train: 0.08365701138973236\n",
            "Starting epoch 36/70, LR = [2.0]\n",
            "--- Epoch 36, Loss on train: 0.08343957364559174\n",
            "Starting epoch 37/70, LR = [2.0]\n",
            "--- Epoch 37, Loss on train: 0.08370108157396317\n",
            "Starting epoch 38/70, LR = [2.0]\n",
            "--- Epoch 38, Loss on train: 0.08570566028356552\n",
            "Starting epoch 39/70, LR = [2.0]\n",
            "--- Epoch 39, Loss on train: 0.08485513180494308\n",
            "Starting epoch 40/70, LR = [2.0]\n",
            "--- Epoch 40, Loss on train: 0.08435691893100739\n",
            "Starting epoch 41/70, LR = [2.0]\n",
            "--- Epoch 41, Loss on train: 0.08258281648159027\n",
            "Starting epoch 42/70, LR = [2.0]\n",
            "--- Epoch 42, Loss on train: 0.08534029871225357\n",
            "Starting epoch 43/70, LR = [2.0]\n",
            "--- Epoch 43, Loss on train: 0.08440403640270233\n",
            "Starting epoch 44/70, LR = [2.0]\n",
            "--- Epoch 44, Loss on train: 0.08099071681499481\n",
            "Starting epoch 45/70, LR = [2.0]\n",
            "--- Epoch 45, Loss on train: 0.08359190076589584\n",
            "Starting epoch 46/70, LR = [2.0]\n",
            "--- Epoch 46, Loss on train: 0.08514989912509918\n",
            "Starting epoch 47/70, LR = [2.0]\n",
            "--- Epoch 47, Loss on train: 0.08213920146226883\n",
            "Starting epoch 48/70, LR = [2.0]\n",
            "--- Epoch 48, Loss on train: 0.08315158635377884\n",
            "Starting epoch 49/70, LR = [2.0]\n",
            "--- Epoch 49, Loss on train: 0.08416211605072021\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.07881160825490952\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.08146923035383224\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.07878845930099487\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.07776006311178207\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.07825755327939987\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.07913519442081451\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.07699315249919891\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.07685824483633041\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.07664564251899719\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.07855241000652313\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.07912591099739075\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.07763039320707321\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.07772784680128098\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.07759737968444824\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.0790599063038826\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.07686005532741547\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.07642703503370285\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.07771570235490799\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.07691317051649094\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.07787464559078217\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.07895026355981827\n",
            "Length on train dataset: 50000\n",
            "Loss on eval (group): 0.0\n",
            "Accuracy on eval (group): 0.3343333333333333\n",
            "Accuracy on train (group): 0.9992\n",
            "Accuracy on test novel classes (group): 0.81\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 7°/10\n",
            "Training set length: 5000\n",
            "Test set length: 7000\n",
            "Starting epoch 1/70, LR = [2.0]\n",
            "--- Initial loss on train: 0.20707625150680542\n",
            "--- Epoch 1, Loss on train: 0.11891218274831772\n",
            "Starting epoch 2/70, LR = [2.0]\n",
            "--- Epoch 2, Loss on train: 0.11239290237426758\n",
            "Starting epoch 3/70, LR = [2.0]\n",
            "--- Epoch 3, Loss on train: 0.1134103536605835\n",
            "Starting epoch 4/70, LR = [2.0]\n",
            "--- Epoch 4, Loss on train: 0.10739081352949142\n",
            "Starting epoch 5/70, LR = [2.0]\n",
            "--- Epoch 5, Loss on train: 0.11089751869440079\n",
            "Starting epoch 6/70, LR = [2.0]\n",
            "--- Epoch 6, Loss on train: 0.10886169970035553\n",
            "Starting epoch 7/70, LR = [2.0]\n",
            "--- Epoch 7, Loss on train: 0.10679689049720764\n",
            "Starting epoch 8/70, LR = [2.0]\n",
            "--- Epoch 8, Loss on train: 0.10841868817806244\n",
            "Starting epoch 9/70, LR = [2.0]\n",
            "--- Epoch 9, Loss on train: 0.10739873349666595\n",
            "Starting epoch 10/70, LR = [2.0]\n",
            "--- Epoch 10, Loss on train: 0.10564542561769485\n",
            "Starting epoch 11/70, LR = [2.0]\n",
            "--- Epoch 11, Loss on train: 0.10205262154340744\n",
            "Starting epoch 12/70, LR = [2.0]\n",
            "--- Epoch 12, Loss on train: 0.10670606046915054\n",
            "Starting epoch 13/70, LR = [2.0]\n",
            "--- Epoch 13, Loss on train: 0.10868961364030838\n",
            "Starting epoch 14/70, LR = [2.0]\n",
            "--- Epoch 14, Loss on train: 0.10461073368787766\n",
            "Starting epoch 15/70, LR = [2.0]\n",
            "--- Epoch 15, Loss on train: 0.10263100266456604\n",
            "Starting epoch 16/70, LR = [2.0]\n",
            "--- Epoch 16, Loss on train: 0.10537547618150711\n",
            "Starting epoch 17/70, LR = [2.0]\n",
            "--- Epoch 17, Loss on train: 0.10608646273612976\n",
            "Starting epoch 18/70, LR = [2.0]\n",
            "--- Epoch 18, Loss on train: 0.10516903549432755\n",
            "Starting epoch 19/70, LR = [2.0]\n",
            "--- Epoch 19, Loss on train: 0.10412219911813736\n",
            "Starting epoch 20/70, LR = [2.0]\n",
            "--- Epoch 20, Loss on train: 0.10205024480819702\n",
            "Starting epoch 21/70, LR = [2.0]\n",
            "--- Epoch 21, Loss on train: 0.10499471426010132\n",
            "Starting epoch 22/70, LR = [2.0]\n",
            "--- Epoch 22, Loss on train: 0.10329198837280273\n",
            "Starting epoch 23/70, LR = [2.0]\n",
            "--- Epoch 23, Loss on train: 0.10180190950632095\n",
            "Starting epoch 24/70, LR = [2.0]\n",
            "--- Epoch 24, Loss on train: 0.10529018193483353\n",
            "Starting epoch 25/70, LR = [2.0]\n",
            "--- Epoch 25, Loss on train: 0.10216349363327026\n",
            "Starting epoch 26/70, LR = [2.0]\n",
            "--- Epoch 26, Loss on train: 0.10154363512992859\n",
            "Starting epoch 27/70, LR = [2.0]\n",
            "--- Epoch 27, Loss on train: 0.10428576916456223\n",
            "Starting epoch 28/70, LR = [2.0]\n",
            "--- Epoch 28, Loss on train: 0.10505691170692444\n",
            "Starting epoch 29/70, LR = [2.0]\n",
            "--- Epoch 29, Loss on train: 0.10261055082082748\n",
            "Starting epoch 30/70, LR = [2.0]\n",
            "--- Epoch 30, Loss on train: 0.1018276959657669\n",
            "Starting epoch 31/70, LR = [2.0]\n",
            "--- Epoch 31, Loss on train: 0.10474950820207596\n",
            "Starting epoch 32/70, LR = [2.0]\n",
            "--- Epoch 32, Loss on train: 0.10254894196987152\n",
            "Starting epoch 33/70, LR = [2.0]\n",
            "--- Epoch 33, Loss on train: 0.10425445437431335\n",
            "Starting epoch 34/70, LR = [2.0]\n",
            "--- Epoch 34, Loss on train: 0.10436996072530746\n",
            "Starting epoch 35/70, LR = [2.0]\n",
            "--- Epoch 35, Loss on train: 0.10162687301635742\n",
            "Starting epoch 36/70, LR = [2.0]\n",
            "--- Epoch 36, Loss on train: 0.10207170993089676\n",
            "Starting epoch 37/70, LR = [2.0]\n",
            "--- Epoch 37, Loss on train: 0.10039712488651276\n",
            "Starting epoch 38/70, LR = [2.0]\n",
            "--- Epoch 38, Loss on train: 0.10383221507072449\n",
            "Starting epoch 39/70, LR = [2.0]\n",
            "--- Epoch 39, Loss on train: 0.10167715698480606\n",
            "Starting epoch 40/70, LR = [2.0]\n",
            "--- Epoch 40, Loss on train: 0.10280946642160416\n",
            "Starting epoch 41/70, LR = [2.0]\n",
            "--- Epoch 41, Loss on train: 0.10344908386468887\n",
            "Starting epoch 42/70, LR = [2.0]\n",
            "--- Epoch 42, Loss on train: 0.10170208662748337\n",
            "Starting epoch 43/70, LR = [2.0]\n",
            "--- Epoch 43, Loss on train: 0.10359121114015579\n",
            "Starting epoch 44/70, LR = [2.0]\n",
            "--- Epoch 44, Loss on train: 0.0967864990234375\n",
            "Starting epoch 45/70, LR = [2.0]\n",
            "--- Epoch 45, Loss on train: 0.10476541519165039\n",
            "Starting epoch 46/70, LR = [2.0]\n",
            "--- Epoch 46, Loss on train: 0.10390857607126236\n",
            "Starting epoch 47/70, LR = [2.0]\n",
            "--- Epoch 47, Loss on train: 0.10050765424966812\n",
            "Starting epoch 48/70, LR = [2.0]\n",
            "--- Epoch 48, Loss on train: 0.10064098238945007\n",
            "Starting epoch 49/70, LR = [2.0]\n",
            "--- Epoch 49, Loss on train: 0.10038164258003235\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.09567713737487793\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.09811412543058395\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.09628918766975403\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.09537944942712784\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.09654618799686432\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.0964084193110466\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.09360888600349426\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.09561354666948318\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.09477292001247406\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.0956784039735794\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.09565145522356033\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.09801923483610153\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.09597297757863998\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.09477851539850235\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.0964188203215599\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.09093701094388962\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.09368995577096939\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.09529295563697815\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.09479857981204987\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.09465938061475754\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.09421823173761368\n",
            "Length on train dataset: 50000\n",
            "Loss on eval (group): 0.0\n",
            "Accuracy on eval (group): 0.3072857142857143\n",
            "Accuracy on train (group): 0.9988\n",
            "Accuracy on test novel classes (group): 0.839\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 8°/10\n",
            "Training set length: 5000\n",
            "Test set length: 8000\n",
            "Starting epoch 1/70, LR = [2.0]\n",
            "--- Initial loss on train: 0.22628794610500336\n",
            "--- Epoch 1, Loss on train: 0.13981378078460693\n",
            "Starting epoch 2/70, LR = [2.0]\n",
            "--- Epoch 2, Loss on train: 0.12810078263282776\n",
            "Starting epoch 3/70, LR = [2.0]\n",
            "--- Epoch 3, Loss on train: 0.12620605528354645\n",
            "Starting epoch 4/70, LR = [2.0]\n",
            "--- Epoch 4, Loss on train: 0.12859883904457092\n",
            "Starting epoch 5/70, LR = [2.0]\n",
            "--- Epoch 5, Loss on train: 0.12555384635925293\n",
            "Starting epoch 6/70, LR = [2.0]\n",
            "--- Epoch 6, Loss on train: 0.12936529517173767\n",
            "Starting epoch 7/70, LR = [2.0]\n",
            "--- Epoch 7, Loss on train: 0.1248856633901596\n",
            "Starting epoch 8/70, LR = [2.0]\n",
            "--- Epoch 8, Loss on train: 0.12414360046386719\n",
            "Starting epoch 9/70, LR = [2.0]\n",
            "--- Epoch 9, Loss on train: 0.12344184517860413\n",
            "Starting epoch 10/70, LR = [2.0]\n",
            "--- Epoch 10, Loss on train: 0.12467345595359802\n",
            "Starting epoch 11/70, LR = [2.0]\n",
            "--- Epoch 11, Loss on train: 0.1264035552740097\n",
            "Starting epoch 12/70, LR = [2.0]\n",
            "--- Epoch 12, Loss on train: 0.1207844689488411\n",
            "Starting epoch 13/70, LR = [2.0]\n",
            "--- Epoch 13, Loss on train: 0.1262124627828598\n",
            "Starting epoch 14/70, LR = [2.0]\n",
            "--- Epoch 14, Loss on train: 0.12424644082784653\n",
            "Starting epoch 15/70, LR = [2.0]\n",
            "--- Epoch 15, Loss on train: 0.1219213530421257\n",
            "Starting epoch 16/70, LR = [2.0]\n",
            "--- Epoch 16, Loss on train: 0.12342247366905212\n",
            "Starting epoch 17/70, LR = [2.0]\n",
            "--- Epoch 17, Loss on train: 0.122691310942173\n",
            "Starting epoch 18/70, LR = [2.0]\n",
            "--- Epoch 18, Loss on train: 0.12331583350896835\n",
            "Starting epoch 19/70, LR = [2.0]\n",
            "--- Epoch 19, Loss on train: 0.12257688492536545\n",
            "Starting epoch 20/70, LR = [2.0]\n",
            "--- Epoch 20, Loss on train: 0.12214701622724533\n",
            "Starting epoch 21/70, LR = [2.0]\n",
            "--- Epoch 21, Loss on train: 0.12138506770133972\n",
            "Starting epoch 22/70, LR = [2.0]\n",
            "--- Epoch 22, Loss on train: 0.115739606320858\n",
            "Starting epoch 23/70, LR = [2.0]\n",
            "--- Epoch 23, Loss on train: 0.11977304518222809\n",
            "Starting epoch 24/70, LR = [2.0]\n",
            "--- Epoch 24, Loss on train: 0.12105649709701538\n",
            "Starting epoch 25/70, LR = [2.0]\n",
            "--- Epoch 25, Loss on train: 0.12185325473546982\n",
            "Starting epoch 26/70, LR = [2.0]\n",
            "--- Epoch 26, Loss on train: 0.12077581882476807\n",
            "Starting epoch 27/70, LR = [2.0]\n",
            "--- Epoch 27, Loss on train: 0.12358517199754715\n",
            "Starting epoch 28/70, LR = [2.0]\n",
            "--- Epoch 28, Loss on train: 0.12341316044330597\n",
            "Starting epoch 29/70, LR = [2.0]\n",
            "--- Epoch 29, Loss on train: 0.12072398513555527\n",
            "Starting epoch 30/70, LR = [2.0]\n",
            "--- Epoch 30, Loss on train: 0.11911112815141678\n",
            "Starting epoch 31/70, LR = [2.0]\n",
            "--- Epoch 31, Loss on train: 0.1172499805688858\n",
            "Starting epoch 32/70, LR = [2.0]\n",
            "--- Epoch 32, Loss on train: 0.1215406209230423\n",
            "Starting epoch 33/70, LR = [2.0]\n",
            "--- Epoch 33, Loss on train: 0.11965343356132507\n",
            "Starting epoch 34/70, LR = [2.0]\n",
            "--- Epoch 34, Loss on train: 0.12016906589269638\n",
            "Starting epoch 35/70, LR = [2.0]\n",
            "--- Epoch 35, Loss on train: 0.12348061054944992\n",
            "Starting epoch 36/70, LR = [2.0]\n",
            "--- Epoch 36, Loss on train: 0.11846543848514557\n",
            "Starting epoch 37/70, LR = [2.0]\n",
            "--- Epoch 37, Loss on train: 0.11760299652814865\n",
            "Starting epoch 38/70, LR = [2.0]\n",
            "--- Epoch 38, Loss on train: 0.12045888602733612\n",
            "Starting epoch 39/70, LR = [2.0]\n",
            "--- Epoch 39, Loss on train: 0.118410624563694\n",
            "Starting epoch 40/70, LR = [2.0]\n",
            "--- Epoch 40, Loss on train: 0.11597409844398499\n",
            "Starting epoch 41/70, LR = [2.0]\n",
            "--- Epoch 41, Loss on train: 0.11905106902122498\n",
            "Starting epoch 42/70, LR = [2.0]\n",
            "--- Epoch 42, Loss on train: 0.12229327857494354\n",
            "Starting epoch 43/70, LR = [2.0]\n",
            "--- Epoch 43, Loss on train: 0.11809785664081573\n",
            "Starting epoch 44/70, LR = [2.0]\n",
            "--- Epoch 44, Loss on train: 0.12026946991682053\n",
            "Starting epoch 45/70, LR = [2.0]\n",
            "--- Epoch 45, Loss on train: 0.12152556329965591\n",
            "Starting epoch 46/70, LR = [2.0]\n",
            "--- Epoch 46, Loss on train: 0.11847414076328278\n",
            "Starting epoch 47/70, LR = [2.0]\n",
            "--- Epoch 47, Loss on train: 0.11784063279628754\n",
            "Starting epoch 48/70, LR = [2.0]\n",
            "--- Epoch 48, Loss on train: 0.11768855899572372\n",
            "Starting epoch 49/70, LR = [2.0]\n",
            "--- Epoch 49, Loss on train: 0.11548938602209091\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.11584535241127014\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.11177361011505127\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.11315019428730011\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.11331208050251007\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.11287093907594681\n",
            "Starting epoch 55/70, LR = [0.4]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}